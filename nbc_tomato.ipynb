{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import csv\nimport random\nimport math",
      "metadata": {
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# creating the function to load the dataset.\ndef LoadDataset(filename):\n    dataset = []\n    with open(filename, 'r') as csvfile:\n        reader = csv.reader(csvfile)\n        for row in reader:\n            dataset.append(row)\n    return dataset",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def reviews_load(filename):\n    reviews = []\n    with open(filename, 'r') as file:\n        reader = csv.DictReader(file)\n        for row in reader:\n            reviews.append(row)\n    return reviews",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\n# then split the dataset into train, dev, and test sets\ndef split_train_dev_eval(dataset, split_ratio):\n    train_size = int(len(dataset) * split_ratio) # split the train according to input ratio\n    train_dataset = []\n    devandeval = dataset[:]\n    while len(train_dataset) < train_size:\n        index = random.randrange(len(devandeval))\n        train_dataset.append(devandeval.pop(index))\n    return [train_dataset, devandeval, devandeval] #return list contains the three parts.",
      "metadata": {
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# function that calculates the word occurence in all reviews\ndef word_occurrences(dataset, vocabulary):\n    WordOccurrences = {}\n    for word in vocabulary:\n        WordOccurrences[word] = 0\n        for document in dataset:\n            if word in document[0].split():\n                WordOccurrences[word] += 1\n    return WordOccurrences",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "\n# function that calculates the probability of each class (fresh or rotten)\ndef ClassProbabilities(dataset):\n\n    class_probabilities = {}\n    \n    for document in dataset:\n        sentiment = document[0]\n        if sentiment == \"Freshness\": # ignoring the excess lables from the dataset.\n            pass\n        else:\n\n            if sentiment not in class_probabilities:\n                class_probabilities[sentiment] = 1 \n            else:\n                class_probabilities[sentiment] += 1 #counting the number of reviews by adding ones in each loop trun.\n    total_documents = len(dataset) #the number of all reviews\n    \n    for sentiment in class_probabilities:\n        class_probabilities[sentiment] /= total_documents\n        \n        \n    return class_probabilities #returns dictionary of both classes probabilities.",
      "metadata": {
        "trusted": true
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# calculating the conditional probability of each word given the class.\ndef word_conditional_probability_smoothing(dataset, vocabulary, word_occurrences,alpha):\n    conditional_probabilities = {}\n    total_words_per_class = {}\n    for sentiment in [\"fresh\", \"rotten\"]:\n        total_words_per_class[sentiment] = sum(word_occurrences[word][sentiment] for word in vocabulary) + alpha * len(\n            vocabulary)\n        for word in vocabulary:\n            if word not in conditional_probabilities:\n                conditional_probabilities[word] = {\n                    sentiment: (word_occurrences[word][sentiment] + alpha) / total_words_per_class[sentiment]}\n            else:\n                conditional_probabilities[word][sentiment] = (word_occurrences[word][sentiment] + alpha) / \\\n                                                             total_words_per_class[sentiment]\n    return conditional_probabilities\n #returns dictionary of each word and its corrseponding probability in both classes.\n    \ndef calculate_conditional_probabilities(dataset, vocabulary, word_occurrences):\n    conditional_probabilities = {}\n    total_words_per_class = {}\n    for sentiment in [\"fresh\",\"rotten\"]:\n        total_words_per_class[sentiment] = 0\n        for word in vocabulary:\n            total_words_per_class[sentiment] += word_occurrences[word][sentiment] + 1\n    for word in vocabulary:\n        conditional_probabilities[word] = {}\n        for sentiment in [\"fresh\",\"rotten\"]:\n            conditional_probabilities[word][sentiment] = (word_occurrences[word][sentiment] + 1) / total_words_per_class[sentiment]\n    return conditional_probabilities\n #returns dictionary of each word and its corrseponding probability in both classes.",
      "metadata": {
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# here we make predictions according to the model we created.\ndef predict(document, vocabulary, class_probabilities, conditional_probabilities):\n    words = document.split()\n    positive_probability = math.log(class_probabilities['fresh'])\n    negative_probability = math.log(class_probabilities['rotten'])\n    for word in words:\n        if word in vocabulary:\n            positive_probability += math.log(conditional_probabilities[word]['fresh'])\n            negative_probability += math.log(conditional_probabilities[word]['rotten'])\n    if positive_probability > negative_probability:\n        return 'fresh'\n    else:\n        return 'rotten'\n   ",
      "metadata": {
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# then lets evaluate our model.\ndef evaluate(dataset, vocabulary, class_probabilities, conditional_probabilities):\n    correct_predictions = 0\n    for document in dataset:\n        predicted_sentiment = predict(document[1], vocabulary, class_probabilities, conditional_probabilities)\n   \n        if predicted_sentiment == document[0]:\n            correct_predictions += 1\n    accuracy = round(correct_predictions / len(dataset),2)\n    return accuracy",
      "metadata": {
        "trusted": true
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# the main function\ndef main():\n    \n    #first calculating the top repeated words.\n  \n    dataset=reviews_load('rt_reviews.csv')\n    word_counts = {}\n    for review in dataset:\n\n        words = review['Review'].split()\n        for word in words:\n            if word in word_counts:\n                word_counts[word] += 1\n            else:\n                word_counts[word] = 1\n    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n    vocab_list=[]\n    for i in range(10000):\n        if not sorted_word_counts[i][1] < 5 : #checking if the word is repeated more than five times.\n            vocab_list.append(sorted_word_counts[i][0]) #buildig vocab list of top ten thousand word in the dataset.\n      \n\n    \n    #loading the dataset  \n    \n    dataset = LoadDataset('rt_reviews.csv')\n    #calculating the\n    word_the_occurrences_in_docs=0\n    for document in dataset:\n        document=document[1].split()\n        if \"the\" in document:\n            word_the_occurrences_in_docs+=1\n\n    Probability_the_occurrence=word_the_occurrences_in_docs/len(dataset)\n    print(\"Probability of 'the' occurrence=\",Probability_the_occurrence)\n\n    cond_prob=0\n    positive_doc=0\n    for document in dataset:\n\n        if document[0]==\"fresh\":\n            positive_doc+=1\n            document=document[1].split()\n            if \"the\" in document:\n                cond_prob+=1\n\n    Probability_conditional = cond_prob / positive_doc\n    print(\"Conditional probability based on the sentiment=\", Probability_conditional)\n    \n    #start preparing the dataset\n    # Split the dataset into train, dev, and test sets\n    train_set, dev_set, test_set = split_train_dev_eval(dataset, 0.6)\n     \n    #calculatingthe word occurrence\n    \n    word_occurrences = {}\n    for word in vocab_list: #     positive     neg1tive\n        word_occurrences[word] = {'fresh': 0, 'rotten': 0}\n        for document in train_set:\n            if word in document[1].split():\n                sentiment = document[0]\n                if sentiment==\"Freshness\":\n                    pass\n                else:\n                    word_occurrences[word][sentiment] += 1\n    # lets do the calculations\n    # Calculate ClassProbabilities\n    class_probabilities = ClassProbabilities(train_set)\n    print(\"Class Probabilities:\",class_probabilities) # prior probability.\n    alpha=100 #adding the smoothing way\n    \n    conditional_probabilities = calculate_conditional_probabilities(train_set, vocab_list, word_occurrences)\n    \n    conditional_probabilities_smoothing = word_conditional_probability_smoothing(train_set, vocab_list, word_occurrences,alpha)\n    \n    # Evaluate the accuracy of the classifier on the development set\n    dev_accuracy = evaluate(dev_set, vocab_list, class_probabilities, conditional_probabilities)\n  \n    dev_accuracy_smoothing = evaluate(dev_set, vocab_list, class_probabilities, conditional_probabilities_smoothing)\n    \n    effect_of_smoothing=abs(dev_accuracy_smoothing-dev_accuracy)\n    \n    print(\"Accuracy of dev set:\",dev_accuracy)\n    print(\"Effect of Smoothing on dev set:\",effect_of_smoothing)\n    \n    test_accuracy_smoothing = evaluate(test_set, vocab_list, class_probabilities, conditional_probabilities_smoothing) #conducting the accuracy on test set using hyperparameters.\n    print(\"Accuracy of test set using optimal hyperparameters:\",test_accuracy_smoothing)\n    \n    \n    word_counts_fresh = {}\n    word_counts_rotten = {}\n    dataset=reviews_load('rt_reviews.csv')\n    for review in dataset:\n        if review[\"Freshness\"]==\"fresh\":\n            words = review['Review'].split()\n            for word in words:\n                if word in word_counts_fresh:\n                    word_counts_fresh[word] += 1\n                else:\n                    word_counts_fresh[word] = 1\n        else:\n            words = review['Review'].split()\n            for word in words:\n                if word in word_counts_rotten:\n                    word_counts_rotten[word] += 1\n                else:\n                    word_counts_rotten[word] = 1\n                    \n    sorted_word_counts_fresh = sorted(word_counts_fresh.items(), key=lambda x: x[1], reverse=True)\n    sorted_word_counts_rotten = sorted(word_counts_rotten.items(), key=lambda x: x[1], reverse=True)\n    vocab_fresh=[]\n    vocab_rotten=[]\n    for i in range(10):\n        vocab_fresh.append(sorted_word_counts_fresh[i][0])\n        vocab_rotten.append(sorted_word_counts_rotten[i][0])\n        \n    print(\"Top 10 most frequent words:\")\n    print(\"In Fresh:\",vocab_fresh)\n    print(\"In Rotten:\",vocab_rotten)\n    \n# run the code\nmain()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "text": "Probability of 'the' occurrence= 0.5705550613436222\nConditional probability based on the sentiment= 0.5793541666666666\nClass Probabilities: {'rotten': 0.4987326388888889, 'fresh': 0.5012673611111111}\nAccuracy of dev set: 0.77\nEffect of Smoothing on dev set: 0.020000000000000018\nAccuracy of test set using optimal hyperparameters: 0.75\nTop 10 most frequent words:\nIn Fresh: ['the', 'and', 'a', 'of', 'to', 'is', 'in', 'that', 'with', 'it']\nIn Rotten: ['the', 'a', 'of', 'and', 'to', 'is', 'in', 'that', 'it', 'The']\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}